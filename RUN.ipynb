{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch import functional as F\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchinfo import torchinfo\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import albumentations as A\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torchmetrics\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import cv2\n",
    "import os, glob\n",
    "\n",
    "from utils.networks import CNN, EfficientUNet_B0\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FACERECOG_PATH = 'Models/mask_model.pth'\n",
    "INFILLER_PATH  = 'Models/efficientnet_tuned.pth'\n",
    "\n",
    "\n",
    "def load_file(path, device = device):\n",
    "    with open(path, 'rb') as f:\n",
    "        return torch.load(f, map_location = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the mask-detection model\n",
    "\n",
    "classifier = CNN().to(device)\n",
    "classifier.load_state_dict(load_file(FACERECOG_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the inpainting model\n",
    "\n",
    "infiller = EfficientUNet_B0().to(device)\n",
    "infiller.load_state_dict(load_file(INFILLER_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier.eval()\n",
    "infiller.eval()\n",
    "\n",
    "classifier.requires_grad_(False)\n",
    "infiller.requires_grad_(False)\n",
    "\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resize_transformation = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0.5, 0.5),\n",
    "    transforms.Resize((64, 64), antialias = True)\n",
    "])\n",
    "\n",
    "def masked_inference_pipeline(image):\n",
    "    with torch.no_grad():\n",
    "        img = resize_transformation(image)[None, ...]\n",
    "        img = img.to(device)\n",
    "        return classifier(img).item() <= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infill_transformation = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((64, 64), antialias = True)\n",
    "])\n",
    "\n",
    "infiller_back_to_array = transforms.Compose([\n",
    "    \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infilling_pipeline(image, original_shape):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    with torch.no_grad():\n",
    "        img = infill_transformation(image)[None, ...]\n",
    "\n",
    "        img = img.to(device)\n",
    "        img = infiller(img)[0]\n",
    "        img = transforms.Resize(original_shape, antialias = False)(img)\n",
    "        img =  img.detach().cpu().permute(1,2,0).numpy()\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
    "    return img*255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "nose_cascade = cv2.CascadeClassifier( cv2.data.haarcascades + 'haarcascade_mcs_nose.xml')\n",
    "\n",
    "def getMask(img):\n",
    "  # Load the image\n",
    "  # img = cv2.imread(imagePath)\n",
    "  # Convert the image to grayscale\n",
    "  gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "  # Detect faces\n",
    "  faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "  # Loop through each face and detect eyes\n",
    "\n",
    "  # Create a mask image\n",
    "  mask = np.ones(img.shape[:2], np.uint8)\n",
    "  \n",
    "  facesCount = 0\n",
    "  for (x,y,w,h) in faces:\n",
    "\n",
    "      roi_gray = gray[y:y+h, x:x+w]\n",
    "      roi_color = img[y:y+h, x:x+w]\n",
    "      eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "      \n",
    "      # Loop through each eye and get the coordinates\n",
    "      # eye_y = img.shape[0]\n",
    "      cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 4)        # create bounding box for the face\n",
    "      eyeCount = len(eyes)\n",
    "      lowesty = 0\n",
    "      for (ex,ey,ew,eh) in eyes:\n",
    "          eye_x = x + ex + int(ew/2)\n",
    "          eye_y = y + ey + int(eh/2)  \n",
    "          lowesty = max(lowesty, eye_y + eh//4)\n",
    "        #   cv2.rectangle(img, (eye_x-ew//2, eye_y-eh//2), (eye_x+ew//2, eye_y+eh//2), (0, 0, 255), 4)\n",
    "      \n",
    "      face_img = img[y:y+h, x:x+w]\n",
    "    \n",
    "      #masked_image = cv2.bitwise_and(img, img, mask = mask)\n",
    "      \n",
    "      # masked or not\n",
    "      facesCount += 1\n",
    "        \n",
    "      if(eyeCount >= 1 and masked_inference_pipeline(face_img)):\n",
    "          \n",
    "          # cv2.imshow(face_img)\n",
    "          # cv2.waitKey(0)\n",
    "          # for posx in range(x, x+w):\n",
    "          #   for posy in range(lowesty, min(y+h+h//12, mask.shape[0])):\n",
    "          mask[lowesty: min(y+h+h//12, mask.shape[0]), x:x+w] = 0\n",
    "          masked_image = cv2.bitwise_and(img, img, mask = mask)\n",
    "\n",
    "          face_img = masked_image[y:y+h, x:x+w]\n",
    "          original_shape = face_img.shape[:2]\n",
    "\n",
    "          # infilling\n",
    "          infill = infilling_pipeline(face_img, original_shape)\n",
    "\n",
    "          img[y:y+h, x:x+w, :] = infill\n",
    "\n",
    "\n",
    "      if(facesCount > 0):\n",
    "        break\n",
    "\n",
    "  #masked_image = cv2.bitwise_and(img, img, mask = mask) \n",
    "  return img\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    # Read the frame\n",
    "    _, img = cap.read()\n",
    "    gray = getMask(img)\n",
    "    cv2.imshow('img', gray)\n",
    "    k = cv2.waitKey(30) & 0xff\n",
    "    if k==27:\n",
    "        break\n",
    "\n",
    "\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unmasked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "# face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "# eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "# nose_cascade = cv2.CascadeClassifier( cv2.data.haarcascades + 'haarcascade_mcs_nose.xml')\n",
    "\n",
    "# def getMask(img):\n",
    "#   # Load the image\n",
    "#   # img = cv2.imread(imagePath)\n",
    "#   # Convert the image to grayscale\n",
    "#   gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#   # Detect faces\n",
    "#   faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "#   # Loop through each face and detect eyes\n",
    "\n",
    "#   # Create a mask image\n",
    "#   mask = np.ones(img.shape[:2], np.uint8)\n",
    "#   for (x,y,w,h) in faces:\n",
    "#       roi_gray = gray[y:y+h, x:x+w]\n",
    "#       roi_color = img[y:y+h, x:x+w]\n",
    "#       eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "      \n",
    "#       # Loop through each eye and get the coordinates\n",
    "#       # eye_y = img.shape[0]\n",
    "#       cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 4)\n",
    "#       eyeCount = len(eyes)\n",
    "#       lowesty = 0\n",
    "#       for (ex,ey,ew,eh) in eyes:\n",
    "#           eye_x = x + ex + int(ew/2)\n",
    "#           eye_y = y + ey + int(eh/2)  \n",
    "#           lowesty = max(lowesty, eye_y + eh//3)\n",
    "#           cv2.rectangle(img, (eye_x-ew//2, eye_y-eh//2), (eye_x+ew//2, eye_y+eh//2), (0, 0, 255), 4)\n",
    "        \n",
    "#       if(eyeCount >= 1 and True):\n",
    "#         for posx in range(x, x+w):\n",
    "#           for posy in range(lowesty, y+h):\n",
    "#             mask[posy][posx] = 0\n",
    "#   masked_image = cv2.bitwise_and(img, img, mask=mask)\n",
    "#   # Display the image\n",
    "#   return masked_image\n",
    "\n",
    "\n",
    "# cap = cv2.VideoCapture(0)\n",
    "# while True:\n",
    "#     # Read the frame\n",
    "#     _, img = cap.read()\n",
    "#     gray = getMask(img)\n",
    "#     cv2.imshow('img', gray)\n",
    "#     k = cv2.waitKey(30) & 0xff\n",
    "#     if k==27:\n",
    "#         break\n",
    "\n",
    "\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('cvproject')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8e9b273093b79eabda493c170a15e7ff42f4736405ada6f2b68f061601f943a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
