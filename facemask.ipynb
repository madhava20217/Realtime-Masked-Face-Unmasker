{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\cvproject\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from util.image import unnormalize\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch import functional as F\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchinfo import torchinfo\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import albumentations as A\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torchmetrics\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import cv2\n",
    "from util.io import load_ckpt\n",
    "\n",
    "from util.loss import  InpaintingLoss\n",
    "import os, glob\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FACERECOG_PATH = './mask_identification_model/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "weight_decay is not a valid argument, kwargs should be empty  for `optimizer_experimental.Optimizer`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m LOADED_MODEL \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeras\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodels\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mFACERECOG_PATH\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpredict_mask\u001b[39m(input_image):\n\u001b[0;32m      3\u001b[0m     image_resized \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mresize(input_image, (\u001b[38;5;241m64\u001b[39m, \u001b[38;5;241m64\u001b[39m))\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\cvproject\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\ProgramData\\Anaconda3\\envs\\cvproject\\lib\\site-packages\\keras\\optimizers\\optimizer_experimental\\optimizer.py:115\u001b[0m, in \u001b[0;36m_BaseOptimizer._process_kwargs\u001b[1;34m(self, kwargs)\u001b[0m\n\u001b[0;32m    109\u001b[0m     logging\u001b[39m.\u001b[39mwarning(\n\u001b[0;32m    110\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m is deprecated in `optimizer_experimental.Optimizer`\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    111\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m, please check the docstring for valid arguments.\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    112\u001b[0m         k,\n\u001b[0;32m    113\u001b[0m     )\n\u001b[0;32m    114\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 115\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m    116\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mk\u001b[39m}\u001b[39;00m\u001b[39m is not a valid argument, kwargs should be empty \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    117\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m for `optimizer_experimental.Optimizer`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    118\u001b[0m     )\n",
      "\u001b[1;31mTypeError\u001b[0m: weight_decay is not a valid argument, kwargs should be empty  for `optimizer_experimental.Optimizer`."
     ]
    }
   ],
   "source": [
    "LOADED_MODEL = tf.keras.models.load_model(FACERECOG_PATH)\n",
    "def predict_mask(input_image):\n",
    "    image_resized = cv2.resize(input_image, (64, 64))\n",
    "    gray_image = cv2.cvtColor(image_resized, cv2.COLOR_BGR2GRAY)\n",
    "#   face = face_detector.detectMultiScale(gray_image, minNeighbors= 4,flags = cv2.CASCADE_FIND_BIGGEST_OBJECT,minSize=(10, 10))\n",
    "#   for (x, y, w, h) in face:\n",
    "#     cv2.rectangle(image_resized, (x, y), (x+w, y+h), (255,0,0), 1)\n",
    "#     face_image = image_resized[y:y+h, x:x+w]\n",
    "#     face_image = cv2.resize(face_image, (32, 32))\n",
    "#     face_image = face_image.reshape(1,32,32,3)\n",
    "    prediction = LOADED_MODEL.predict(input_image)\n",
    "    if(prediction > 0.5):\n",
    "      return 1\n",
    "    else:\n",
    "      return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 50\u001b[0m\n\u001b[0;32m     48\u001b[0m gray \u001b[38;5;241m=\u001b[39m getMask(img)\n\u001b[0;32m     49\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg\u001b[39m\u001b[38;5;124m'\u001b[39m, gray)\n\u001b[1;32m---> 50\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xff\u001b[39m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m k\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m27\u001b[39m:\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "nose_cascade = cv2.CascadeClassifier( cv2.data.haarcascades + 'haarcascade_mcs_nose.xml')\n",
    "\n",
    "def getMask(img):\n",
    "  # Load the image\n",
    "  # img = cv2.imread(imagePath)\n",
    "  # Convert the image to grayscale\n",
    "  gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "  # Detect faces\n",
    "  faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "  # Loop through each face and detect eyes\n",
    "\n",
    "  # Create a mask image\n",
    "  mask = np.ones(img.shape[:2], np.uint8)\n",
    "  for (x,y,w,h) in faces:\n",
    "      roi_gray = gray[y:y+h, x:x+w]\n",
    "      roi_color = img[y:y+h, x:x+w]\n",
    "      eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "      \n",
    "      # Loop through each eye and get the coordinates\n",
    "      # eye_y = img.shape[0]\n",
    "      cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 4)\n",
    "      eyeCount = len(eyes)\n",
    "      lowesty = 0\n",
    "      for (ex,ey,ew,eh) in eyes:\n",
    "          eye_x = x + ex + int(ew/2)\n",
    "          eye_y = y + ey + int(eh/2)  \n",
    "          lowesty = max(lowesty, eye_y + eh//3)\n",
    "          cv2.rectangle(img, (eye_x-ew//2, eye_y-eh//2), (eye_x+ew//2, eye_y+eh//2), (0, 0, 255), 4)\n",
    "        \n",
    "      if(eyeCount >= 1 and True):\n",
    "        for posx in range(x, x+w):\n",
    "          for posy in range(lowesty, y+h):\n",
    "            mask[posy][posx] = 0\n",
    "  masked_image = cv2.bitwise_and(img, img, mask=mask)\n",
    "  # Display the image\n",
    "  return masked_image\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    # Read the frame\n",
    "    _, img = cap.read()\n",
    "    gray = getMask(img)\n",
    "    cv2.imshow('img', gray)\n",
    "    k = cv2.waitKey(30) & 0xff\n",
    "    if k==27:\n",
    "        break\n",
    "\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('cvproject')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8e9b273093b79eabda493c170a15e7ff42f4736405ada6f2b68f061601f943a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
