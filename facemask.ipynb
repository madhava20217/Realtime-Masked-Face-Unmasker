{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\ProgramData\\Anaconda3\\envs\\cvproject\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import pathlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from util.image import unnormalize\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch import nn\n",
    "from torch import functional as F\n",
    "\n",
    "from torchvision import transforms\n",
    "from torchinfo import torchinfo\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import albumentations as A\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "import torchmetrics\n",
    "from torchvision.utils import save_image, make_grid\n",
    "import cv2\n",
    "from util.io import load_ckpt\n",
    "\n",
    "from util.loss import  InpaintingLoss\n",
    "import os, glob\n",
    "\n",
    "from networks import CNN, EfficientUNet\n",
    "\n",
    "\n",
    "device = torch.device('cuda:0') if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "FACERECOG_PATH = 'mask_model.pth'\n",
    "INFILLER_PATH  = 'efficientunet_temp.pth'\n",
    "\n",
    "\n",
    "def load_file(path):\n",
    "    with open(path, 'rb') as f:\n",
    "        return torch.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# loading the mask-detection model\n",
    "\n",
    "model = CNN().to(device)\n",
    "model.load_state_dict(load_file(FACERECOG_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_img_inference: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Masked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "nose_cascade = cv2.CascadeClassifier( cv2.data.haarcascades + 'haarcascade_mcs_nose.xml')\n",
    "\n",
    "def getMask(img):\n",
    "  # Load the image\n",
    "  # img = cv2.imread(imagePath)\n",
    "  # Convert the image to grayscale\n",
    "  gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "  # Detect faces\n",
    "  faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "  # Loop through each face and detect eyes\n",
    "\n",
    "  # Create a mask image\n",
    "  mask = np.ones(img.shape[:2], np.uint8)\n",
    "  \n",
    "  facesCount = 0\n",
    "  for (x,y,w,h) in faces:\n",
    "\n",
    "      roi_gray = gray[y:y+h, x:x+w]\n",
    "      roi_color = img[y:y+h, x:x+w]\n",
    "      eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "      \n",
    "      # Loop through each eye and get the coordinates\n",
    "      # eye_y = img.shape[0]\n",
    "      cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 4)        # create bounding box for the face\n",
    "      eyeCount = len(eyes)\n",
    "      lowesty = 0\n",
    "      for (ex,ey,ew,eh) in eyes:\n",
    "          eye_x = x + ex + int(ew/2)\n",
    "          eye_y = y + ey + int(eh/2)  \n",
    "          lowesty = max(lowesty, eye_y + eh//4)\n",
    "        #   cv2.rectangle(img, (eye_x-ew//2, eye_y-eh//2), (eye_x+ew//2, eye_y+eh//2), (0, 0, 255), 4)\n",
    "        \n",
    "      face_img = img[y:y+h, x:x+w]\n",
    "\n",
    "\n",
    "        \n",
    "        # Display the cropped face image\n",
    "      \n",
    "      if(eyeCount >= 1 and True):\n",
    "        facesCount += 1\n",
    "        # cv2.imshow(face_img)\n",
    "        # cv2.waitKey(0)\n",
    "        for posx in range(x, x+w):\n",
    "          for posy in range(lowesty, min(y+h+h//12, mask.shape[0])):\n",
    "            mask[posy][posx] = 0\n",
    "      if(facesCount > 0):\n",
    "        break\n",
    "\n",
    "  masked_image = cv2.bitwise_and(img, img, mask = mask) \n",
    "  return masked_image\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "while True:\n",
    "    # Read the frame\n",
    "    _, img = cap.read()\n",
    "    gray = getMask(img)\n",
    "    cv2.imshow('img', gray)\n",
    "    if cv2.waitKey(0):\n",
    "        break\n",
    "\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Unmasked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 50\u001b[0m\n\u001b[0;32m     48\u001b[0m gray \u001b[38;5;241m=\u001b[39m getMask(img)\n\u001b[0;32m     49\u001b[0m cv2\u001b[38;5;241m.\u001b[39mimshow(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimg\u001b[39m\u001b[38;5;124m'\u001b[39m, gray)\n\u001b[1;32m---> 50\u001b[0m k \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwaitKey\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;241m&\u001b[39m \u001b[38;5;241m0xff\u001b[39m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m k\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m27\u001b[39m:\n\u001b[0;32m     52\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import cv2\n",
    "# import numpy as np\n",
    "# face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "# eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "# nose_cascade = cv2.CascadeClassifier( cv2.data.haarcascades + 'haarcascade_mcs_nose.xml')\n",
    "\n",
    "# def getMask(img):\n",
    "#   # Load the image\n",
    "#   # img = cv2.imread(imagePath)\n",
    "#   # Convert the image to grayscale\n",
    "#   gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "#   # Detect faces\n",
    "#   faces = face_cascade.detectMultiScale(gray, 1.3, 5)\n",
    "#   # Loop through each face and detect eyes\n",
    "\n",
    "#   # Create a mask image\n",
    "#   mask = np.ones(img.shape[:2], np.uint8)\n",
    "#   for (x,y,w,h) in faces:\n",
    "#       roi_gray = gray[y:y+h, x:x+w]\n",
    "#       roi_color = img[y:y+h, x:x+w]\n",
    "#       eyes = eye_cascade.detectMultiScale(roi_gray)\n",
    "      \n",
    "#       # Loop through each eye and get the coordinates\n",
    "#       # eye_y = img.shape[0]\n",
    "#       cv2.rectangle(img, (x, y), (x + w, y + h), (0, 255, 0), 4)\n",
    "#       eyeCount = len(eyes)\n",
    "#       lowesty = 0\n",
    "#       for (ex,ey,ew,eh) in eyes:\n",
    "#           eye_x = x + ex + int(ew/2)\n",
    "#           eye_y = y + ey + int(eh/2)  \n",
    "#           lowesty = max(lowesty, eye_y + eh//3)\n",
    "#           cv2.rectangle(img, (eye_x-ew//2, eye_y-eh//2), (eye_x+ew//2, eye_y+eh//2), (0, 0, 255), 4)\n",
    "        \n",
    "#       if(eyeCount >= 1 and True):\n",
    "#         for posx in range(x, x+w):\n",
    "#           for posy in range(lowesty, y+h):\n",
    "#             mask[posy][posx] = 0\n",
    "#   masked_image = cv2.bitwise_and(img, img, mask=mask)\n",
    "#   # Display the image\n",
    "#   return masked_image\n",
    "\n",
    "\n",
    "# cap = cv2.VideoCapture(0)\n",
    "# while True:\n",
    "#     # Read the frame\n",
    "#     _, img = cap.read()\n",
    "#     gray = getMask(img)\n",
    "#     cv2.imshow('img', gray)\n",
    "#     k = cv2.waitKey(30) & 0xff\n",
    "#     if k==27:\n",
    "#         break\n",
    "\n",
    "\n",
    "# cap.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.0 ('cvproject')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "8e9b273093b79eabda493c170a15e7ff42f4736405ada6f2b68f061601f943a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
